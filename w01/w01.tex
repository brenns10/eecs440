\documentclass[fleqn]{homework}

\student{Stephen Brennan (smb196)}
\course{EECS 440}
\assignment{Written Homework 1}
\duedate{September 9, 2015}

\usepackage{amsmath}
%\usepackage{mathtools}
%\usepackage{graphicx}

\begin{document}
  \maketitle

  \begin{problem}{1}
    \begin{question}
      For three random variables $A$, $B$, and $C$, show with a counterexample
      that the statement that ``$A$ is independent of $B$'' does not imply the
      statement ``$A$ is independent of $B$ given $C$.'' (10 points)
    \end{question}

    Let $A$ represent the outcome of a fair coin flip, $B$ represent the outcome
    of a second coin flip, and $C$ represent the indicator that only one of
    coins $A$ and $B$ was heads.  Since $A$ and $B$ represent the outcomes of
    two distinct, fair coins, they are independent.  However, given that $C=1$
    (that is, only one of the two coins was heads), $A$ and $B$ are no longer
    independent: $p(A=h|C=1) = 0.5$, $p(B=h|C=1) = 0.5$, but
    $p(A=h, B=h | C=1) = 0 \neq 0.25$.
  \end{problem}

  \begin{problem}{2}
    \begin{question}
      Points are sampled uniformly at random from the interval $(0,1)^2$ so that
      they lie on the line $x+y=1$.  Determine the expected squared distance
      between any two sampled points. (10 points)
    \end{question}

    We define $X_1$ and $X_2$ to be the $x$-coordinates of the two points
    respectively.  They are both distributed uniformly across $(0, 1)$.  The
    squared distance between these two points is:

    \begin{equation}
      (X_1 - X_2)^2 + (1 - X_1 - 1 + X_2)^2 = 2(X_1 - X_2)^2 = 2X_1^2 - 4X_1X_2 + 2X_2^2
    \end{equation}

    Since expectation is linear, it can be computed separately for each term.
    $E(2X_1^2) = 2 \int_0^1 \! x_1^2 \,\mathrm{d} x = \frac{2}{3}$.
    $E(2X_2^2) = \frac{2}{3}$ as well, since $X_1$ and $X_2$ are i.i.d.  Also
    due to this, we can compute
    $E(4X_1X_2) = 4 E(X_1) E(X_2) = 4 * 0.5 * 0.5 = 1$.  Therefore, we have the
    expected squared distance being
    $\frac{2}{3} - 1 + \frac{2}{3} = \frac{1}{3}$.
  \end{problem}

  \begin{problem}{3}
    \begin{question}
      For any two random variables $X$ and $Y$, the \textbf{conditional
        expectation} of $X$ given $Y=y$ is defined by
      $E(X|Y=y) = \sum x p_x(x|Y=y)$ for a fixed $y$.  Show than, for any three
      random variables $A$, $B$, and $C$, $E(A+B|C=c) = E(A|C=c) +
      E(B|C=c)$. (10 points)
    \end{question}

    \begin{align*}
      E(A + B | C=c) &= \sum_{a\in A} \sum_{b\in B} (a+b)p(A=a,B=b|C=c) \\
                     &= \sum_{a\in A} \sum_{b\in B} \left(a p(A=a,B=b|C=c) + b p(A=a,B=b|C=c) \right) \\
                     &= \sum_{a\in A} \sum_{b\in B} a p(A=a,B=b|C=c) + \sum_{b\in B} \sum_{a\in A} b p(A=a,B=b|C=c) \\
                     &= \sum_{a\in A} \sum_{b\in B} \frac{ap(A=a,B=b,C=c)}{p(C=c)} + \sum_{b\in B} \sum_{a\in A} \frac{bp(A=a,B=b,C=c)}{p(C=c)} \\
                     &= \sum_{a\in A} \frac{ap(A=a,C=c)}{p(C=c)} + \sum_{b\in B} \frac{bp(B=b,C=c)}{p(C=c)} \\
                     &= \sum_{a\in A} a p_A(a|C=c) + \sum_{b\in B} bp_B(b|C=c) \\
                     &= E(A|C=c) + E(B|C=c) \\
    \end{align*}
  \end{problem}

  \begin{problem}{4}
    \begin{question}
      A new, rare disease has arisen that affects 1 in every 100,000 people.  A
      medical test has been invented to diagnose the disease.  For any person
      with the disease, the test successfully diagnoses the disease with
      probability 0.98.  However, if someone does not have the disease, the test
      misdiagnoses them as having the disease with probability 0.002 (called the
      \textbf{false positive rate}).  Person $X$ is tested and the test
      indicates $X$ has the disease.  What is the probability that $X$ actually
      has the disease? (10 points)
    \end{question}

    Let $D$ be the event that person $X$ has the disease, and $T$ be the event
    that the test diagnoses person $X$ with the disease.  Both take on values of
    $t$ or $f$ denoting true or false, respectively.  We are given that
    $p(T=t|D=t) = 0.98$, $p(T=t|D=f)=0.002$, and $p(D=t) = 0.00001$.  We compute
    $p(D=t|T=t)$ using Bayes' Rule:

    \begin{align*}
      p(D=t|T=t) &= \frac{p(T=t|D=t)p(D=t)}{p(T=t|D=t)p(D=t) + p(T=t|D=f)p(D=f)}\\
                 &= \frac{0.98 \times 0.00001}{0.98 \times 0.00001 + 0.002 \times 0.9999}\\
                 &= 4.88 \times 10^{-3}
    \end{align*}
  \end{problem}

  \begin{problem}{5}
    \begin{question}
      Every package of some unhealthy cereal includes an exciting and shiny
      plastic animal.  There are $c$ different types of animal, and each package
      is equally likely to contain any type.  Your children make you buy one
      package of the cereal daily until they have collected all the animals.
      Find the expected number of days that elapse before you can stop buying
      this cereal. (10 points)
    \end{question}
  \end{problem}

\end{document}