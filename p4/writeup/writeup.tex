\documentclass[fleqn]{homework}

\student{Stephen Brennan (smb196)}
\course{EECS 440}
\assignment{Programming 4}
\duedate{November 19, 2015}

%\usepackage{mathtools}
%\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{placeins}

\begin{document}
  \maketitle

  \section{Implementation Commentary}

  For the most part, implementing bagging and boosting was rather simple.  My
  original implementation of decision trees had to be slightly optimized in
  order to provide reasonable performance in this situation.  In particular, my
  selection of a cutoff for continuous variables was inefficient originally: it
  recomputed mutual information (an linear time operation in the number of
  examples) for every potential cutoff (which is also linear in the number of
  examples).  This gave an overall quadratic runtime.  I was able to improve
  this by doing a single iteration over the sorted values for the continuous
  feature.  For each variable, I updated counts (more accurately, weights) of
  examples on either side of the split, and incrementally recomputed the mutual
  information, resulting in a linear time operation.  Accounting for the time to
  sort the examples by their attribute value, this resulted in an $O(n \log n)$
  improvement, which was very noticeable.

  Implementing weights for decision trees was simple.  Most of the modifications
  were the mutual information procedure (using weights instead of counts for
  computing probabilities).  The other critical change was to select the
  majority class label (at leaves) based on the total weights, not on the
  counts.  When I did this, Boosting started giving better results.

  Implementing weights for neural networks was also rather simple.  Instead of
  using the squared loss function $L(\hat{y_i}, y_i) = (\hat{y}_i - y_i)^2$, I
  used a weighted squared loss function:
  $L(\hat{y_i}, y_i) = w_i (\hat{y}_i - y_i)^2$.  Thus, the weights were simply
  multiplied into the gradient for each example.

  Also, it is worth noting that my implementations and changes only went so far
  as the assignment description specified.  That is, I have not tested and will
  not certify that these work when the parameters (for instance, decision tree
  depth or neural net layers) are set to values not specified from the
  assignment description.  So, when using my code, please use the following
  arguments:

  \begin{itemize}
  \item For decision trees, be sure to set \texttt{--depth} to 2 (this means
    level of decision nodes, and then leaf nodes).
  \item For neural networks, be sure to set \texttt{--layer\_sizes} to 0 (no
    hidden layer) and \texttt{--max\_iters} to something reasonable (I used 5000
    in all my tests), since convergence never seems to be a guarantee in my
    neural network implementation.
  \end{itemize}

    \FloatBarrier
  \begin{problem}{a}
    \begin{question}
      For all problems and the two learning algorithms, compare the accuracy of
      the ensemble versions (10 iterations) to the base learners. Produce a
      table with the accuracies of the base learner, the bagged learner, and the
      boosted learner. Perform paired $t$-tests to determine if any of the
      ensemble methods are significantly better than the base learner with 95\%
      confidence.
    \end{question}
    \FloatBarrier

    \begin{table}[h]
    \centering
    \caption{Comparison of base, bagged, and boosted learners.}
    \label{t:a-comparison}
    \begin{tabular}{llll|lll}
      \toprule
      Problem & \multicolumn{3}{c}{Decision Tree} & \multicolumn{3}{c}{Neural Network} \\
      \midrule
             & Base          & Bagging       & Boosting      & Base          & Bagging      & Boosting       \\
      Voting & 0.989 (0.007) & 0.982 (0.015) & 0.984 (0.012) & 0.984 (0.012) & 0.986 (0.009) & 0.977 (0.012) \\
      Spam   & 0.711 (0.002) & 0.710 (0.002) & 0.717 (0.005) & 0.647 (0.034) & 0.657 (0.035) & 0.697 (0.003) \\
      Volcanoes&0.672 (0.001)& 0.672 (0.001) & 0.773 (0.018) & 0.833 (0.028) & 0.839 (0.014) & 0.820 (0.023) \\
      \bottomrule
    \end{tabular}
    \end{table}

    A comparison of the accuracies is presented in Table~\ref{t:a-comparison}.
    Numbers in parentheses are (uncorrected) standard deviations.  Resulting
    95\% confidence intervals are presented in Table~\ref{t:a-ci}

    \begin{table}[h]
      \centering
      \caption{Confidence intervals for difference of ensemble and base learner means.}
      \label{t:a-ci}
      \begin{tabular}{lcccc}
        \toprule
        Problem   & \multicolumn{2}{c}{Decision Tree} & \multicolumn{2}{c}{Neural Network} \\
        \midrule
                  & Bagging         & Boosting        & Bagging         & Boosting \\
        Voting    & (-0.012, 0.026) & (-0.011, 0.021) & (-0.019, 0.015) & (-0.013, 0.027)\\
        Spam      & (-0.005, 0.007) & (-0.014, 0.002) & (-0.066, 0.046) & (-0.089, -0.011)\\
        Volcanoes & (-0.002, 0.002) & (-0.122, -0.080)& (-0.042, 0.030) & (-0.029, 0.055)\\
        \bottomrule
      \end{tabular}
    \end{table}

    According to the confidence intervals in Table~\ref{t:a-ci}, Boosting
    improved the Decision Tree's performance on Volcanoes significantly, and the
    Neural Network's performance on Spam significantly.
  \end{problem}

    \FloatBarrier
  \begin{problem}{b}
    \begin{question}
      For any two problems and the two learning algorithms, evaluate how the
      accuracy of bagging changes with the number of iterations. Pick at least
      three iteration values between 2 and 50, and plot the accuracy on a
      graph. Do you see any difference by problem? By algorithm?
    \end{question}
    \FloatBarrier

    \begin{table}[h]
      \centering
      \caption{Effect of iterations of bagging on accuracy.}
      \label{t:b}
      \begin{tabular}{ll|ll}
        \toprule
        Problem   & Iterations & Decision Tree Accuracy & Neural Network Accuracy \\
        \midrule
        Voting    & 2          & 0.984 (0.014)          & 0.982 (0.012)            \\
                  & 20         & 0.982 (0.015)          & 0.986 (0.009)            \\
                  & 35         & 0.989 (0.010)          & 0.986 (0.009)            \\
                  & 50         & 0.984 (0.015)          & 0.984 (0.012)            \\
        \midrule
        Volcanoes & 2          & 0.676 (0.007)          & 0.827 (0.015)            \\
                  & 20         & 0.672 (0.001)          & 0.839 (0.014)            \\
                  & 35         & 0.672 (0.001)          & 0.848 (0.014)            \\
                  & 50         & 0.672 (0.001)          & 0.845 (0.027)            \\
        \bottomrule
      \end{tabular}
    \end{table}

    These findings are presented in Figures~\ref{f:b-voting} and
    ~\ref{f:b-volcanoes}.  Please note the $y$ scales on these plots.

    \begin{figure}[h!]
      \centering
      \caption{}
      \label{f:b-voting}
      \includegraphics[width=0.67\textwidth]{b-voting.pdf}
    \end{figure}
    \begin{figure}[h!]
      \centering
      \caption{}
      \label{f:b-volcanoes}
      \includegraphics[width=0.67\textwidth]{b-volcanoes.pdf}
    \end{figure}

    Overall, it seems that bagging did not help with voting very much, probably
    because it already had very good accuracy.  Bagging showed mild improvement
    for the neural network, but not for the decision tree.  I noticed that
    bagging became a majority class label predictor for decision tree on
    volcanoes, when you set the iterations high.  I wonder if this is due to the
    fact that decision trees are in general a rather poor predictor for large
    amounts of continuous attributes like volcanoes.
  \end{problem}

    \FloatBarrier
  \begin{problem}{c}
    \begin{question}
      Repeat (b) for boosting.
    \end{question}
    \FloatBarrier

    \begin{table}[h]
      \centering
      \caption{Effect of iterations of boosting on accuracy.}
      \label{t:c}
      \begin{tabular}{ll|ll}
        \toprule
        Problem   & Iterations & Decision Tree Accuracy & Neural Network Accuracy \\
        \midrule
        Voting    & 2          & 0.989 (0.007)          & 0.966 (0.016)            \\
                  & 20         & 0.984 (0.012)          & 0.977 (0.012)            \\
                  & 35         & 0.986 (0.009)          & 0.975 (0.024)            \\
                  & 50         & 0.986 (0.009)          & 0.984 (0.014)            \\
        \midrule
        Volcanoes & 2          & 0.696 (0.021)          & 0.840 (0.009)            \\
                  & 20         & 0.773 (0.018)          & 0.820 (0.023)            \\
                  & 35         & 0.814 (0.024)          & 0.806 (0.014)            \\
                  & 50         & 0.826 (0.026)          & 0.807 (0.016)            \\
        \bottomrule
      \end{tabular}
    \end{table}

    These findings are presented in Figures~\ref{f:c-voting} and
    \ref{f:c-volcanoes}.  Please note the $y$ scales on these plots.  The
    general trend seems to be here that boosting improves the performance of the
    decision tree on volcanoes rather uniformly, while actually hurting the
    performance of the neural network on volcanoes.  Meanwhile, it has very
    little effect on voting.

    \begin{figure}[h!]
      \centering
      \caption{}
      \label{f:c-voting}
      \includegraphics[width=0.67\textwidth]{c-voting.pdf}
    \end{figure}
    \begin{figure}[h!]
      \centering
      \caption{}
      \label{f:c-volcanoes}
      \includegraphics[width=0.67\textwidth]{c-volcanoes.pdf}
    \end{figure}

  \end{problem}

    \FloatBarrier
  \begin{problem}{d}
    \begin{question}
      Evaluate the sensitivity of bagging to noise as follows. When training,
      after constructing the training sample, flip an exampleâ€™s label with
      probability $p$. Then use this noisy sample in your bagging algorithm and
      evaluate the resulting classifier on the usual (noise free) test set. For
      any two problems and any two learning algorithms, plot a graph with $p$ on
      the $x$-axis and the test-set accuracy of bagging (30 iterations) on the
      $y$-axis. You can use results from the previous questions for a $p=0$
      point. Discuss how resilient bagging is to noise based on your
      observations.
    \end{question}
    \FloatBarrier

    \begin{table}[h]
      \centering
      \caption{Effect of flipping labels on bagging accuracy.}
      \label{t:d}
      \begin{tabular}{ll|ll}
        \toprule
        Problem   & $p$        & Decision Tree Accuracy & Neural Network Accuracy \\
        \midrule
        Voting    & 0.00       & 0.982 (0.015)          & 0.986 (0.009)            \\
                  & 0.01       & 0.982 (0.012)          & 0.984 (0.015)            \\
                  & 0.10       & 0.986 (0.009)          & 0.980 (0.011)            \\
                  & 0.50       & 0.643 (0.178)          & 0.693 (0.050)            \\
        \midrule
        Volcanoes & 0.00       & 0.672 (0.001)          & 0.839 (0.014)            \\
                  & 0.01       & 0.672 (0.001)          & 0.841 (0.023)            \\
                  & 0.10       & 0.672 (0.001)          & 0.818 (0.014)            \\
                  & 0.50       & 0.672 (0.001)          & 0.677 (0.007)            \\
        \bottomrule
      \end{tabular}
    \end{table}

    These findings are presented in Figures~\ref{f:d-voting} and
    ~\ref{f:d-volcanoes}.  Please note the $y$ scales on these plots.

    \begin{figure}[h!]
      \centering
      \caption{}
      \label{f:d-voting}
      \includegraphics[width=0.67\textwidth]{d-voting.pdf}
    \end{figure}
    \begin{figure}[h!]
      \centering
      \caption{}
      \label{f:d-volcanoes}
      \includegraphics[width=0.67\textwidth]{d-volcanoes.pdf}
    \end{figure}

    It appears that bagging is surprisingly resilient to this flipping of
    labels.  For the most part, performance across the board doesn't degrade too
    much even at $p=0.10$, which seems pretty high.  I was expecting much worse
    results, which is why there is such a gap between my $p=0.10$ and $p=0.50$
    points.  The only thing to note here is that the decision tree for volcanoes
    seems to have degraded to a majority class prediction in this test (as in
    others).  It is a really strange thing to see, given that this doesn't occur
    with other datasets.
  \end{problem}

    \FloatBarrier
  \begin{problem}{e}
    \begin{question}
      Repeat (d) for boosting.
    \end{question}
    \FloatBarrier

    \begin{table}[h]
      \centering
      \caption{Effect of flipping labels on boosting accuracy.}
      \label{t:e}
      \begin{tabular}{ll|ll}
        \toprule
        Problem   & $p$        & Decision Tree Accuracy & Neural Network Accuracy \\
        \midrule
        Voting    & 0.00       & 0.984 (0.012)          & 0.977 (0.012)            \\
                  & 0.01       & 0.982 (0.015)          & 0.973 (0.012)            \\
                  & 0.10       & 0.982 (0.009)          & 0.942 (0.020)            \\
                  & 0.50       & 0.927 (0.070)          & 0.795 (0.044)            \\
        \midrule
        Volcanoes & 0.00       & 0.773 (0.018)          & 0.820 (0.023)            \\
                  & 0.01       & 0.799 (0.014)          & 0.808 (0.016)            \\
                  & 0.10       & 0.802 (0.010)          & 0.820 (0.019)            \\
                  & 0.50       & 0.719 (0.007)          & 0.736 (0.011)            \\
        \bottomrule
      \end{tabular}
    \end{table}

    These findings are presented in Figures~\ref{f:e-voting} and
    ~\ref{f:e-volcanoes}.  Please note the $y$ scales on these plots.

    \begin{figure}[h!]
      \centering
      \caption{}
      \label{f:e-voting}
      \includegraphics[width=0.67\textwidth]{e-voting.pdf}
    \end{figure}
    \begin{figure}[h!]
      \centering
      \caption{}
      \label{f:e-volcanoes}
      \includegraphics[width=0.67\textwidth]{e-volcanoes.pdf}
    \end{figure}

    These results are rather surprising.  They seem to indicate that boosting is
    somewhat \textit{more} resilient to noise than bagging.  This is surprising
    because in class bagging was presented as a method that was useful for
    ``averaging out'' noise, whereas boosting was presented as a technique that
    could increase the complexity of a decision surface to match examples that
    were misclassified on the previous iteration.
  \end{problem}

\end{document}