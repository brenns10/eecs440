\documentclass[fleqn]{homework}

\student{Stephen Brennan (smb196)}
\course{EECS 440}
\assignment{Written 5}
\duedate{September 29, 2015}

\usepackage{mathtools}
%\usepackage{graphicx}

\begin{document}
  \maketitle

  \begin{problem}{1}
    \begin{question}
      Can you think of any circumstances when it might be \textit{beneficial} to
      overfit? (5 points)
    \end{question}

    Yes.  If you encountered a situation where you were certain your training
    data were accurate, and that your training data represents pretty much all
    that your classifier will see later, then overfitting would be good.
    However, in situations like this, machine learning is not usually the
    appropriate approach.  When you have this much knowledge about the target
    concept, you're probably better off trying to use this knowledge directly
    instead of training a classifier with it.  Machine learning is most
    applicable for situations where the target concept is unknown, or too
    complex to directly write it, and in these cases overfitting is not
    desirable.
  \end{problem}

  \begin{problem}{2}
    \begin{question}
      Person $X$ wishes to evaluate the performance of a learning algorithm on a
      set of $n$ examples.  $X$ employs the following strategy: Divide the $n$
      examples randomly into two equal-sized disjoint sets, $A$ and $B$.  Then
      train the algorithm on $A$ and evaluate it on $B$.  Repeat the previous
      two steps for $N$ iterations ($N$ large), then average the $N$ performance
      measures obtained.  Is this sound empirical methodology?  Explain why or
      why not. (10 points)
    \end{question}

    No, at least not if person $X$ then tried to use the mean and standard
    deviation obtained from these experiments to represent the performance of
    the algorithm on the whole population.  When you evaluate the performance of
    a learning algorithm, you do so by attempting to estimate the parameters of
    the error rate distribution on all training sets (of some size) from the
    population.  Since you do not have all training sets, you pretend you do by
    using $k$ fold validation.  This can give you maximum likelihood estimates
    of the parameters of the true performance, and a confidence interval.  When
    you repeat the experiment many times, you are really measuring the
    performance of the algorithm on the various subsets of your training sample,
    and so the mean and variance of the $N$ iterations will not be a reliable
    estimate of the true performance of the algorithm.
  \end{problem}

  \begin{problem}{3}
    \begin{question}
      Two classifiers $A$ and $B$ are evaluated on a sample with $P$ positive
      examples and $N$ negative examples, and their ROC graphs are plotted.  It
      is found that the ROC of $A$ \textit{dominates} that of $B$, i.e. for
      every FP rate, TP rate of $A \ge$ TP rate of $B$.  Discuss what the
      relationship is between the precision-recall graphs of $A$ and $B$ on the
      same sample. (10 points)
    \end{question}

    Since $A$ and $B$ are evaluated on the same sample, we know that the
    statement ``TP rate of $A \ge$ TP rate of $B$'' is equivalent to saying
    $TP_A \ge TP_B$.  Since the ROC graph is monotonically increasing, $A$
    dominating $B$ also means that for every TP rate, FP rate of $A \le$ FP rate
    of $B$, which is the same as $FP_A \le FP_B$.

    Since Recall is plotted on the $x$ axis of the Precision-Recall graph, $A$
    will be to the right of $B$ (since we already have that $TPR_A \ge TPR_B$.
    Precision is defined as $\frac{TP}{TP+FP}$, which can be equivalently
    written as:

    \begin{equation*}
      \frac{1}{1+\frac{FP}{TP}}
    \end{equation*}

    Since $FP_A \le FP_B$ and $TP_A \ge TP_B$, $Precision_A \ge Precision_B$,
    and therefore the Precision-Recall plot of $A$ will be above and to the
    right of $B$ (whenever they differ, that is).
  \end{problem}

  \begin{problem}{4}
    \begin{question}
      Explain why: \textbf{(i)} an ROC graph must be monotonically increasing,
      \textbf{(ii)} the ROC graph of a majority class classifier is a diagonal
      line, \textbf{(iii)} the ROC graph of a random classifier that ignores
      attributes and guesses each class with equal probability is a diagonal
      line. (15 points)
    \end{question}

    \textbf{(i)} In order to create an ROC graph, you vary the confidence cutoff
    for predictions, and plot the TPR on the $y$ axis, and FPR on the $x$ axis.
    Each prediction has a constant cutoff.  Whenever the FPR increases, the
    cutoff must have been lowered.  This means that the TPR must have either
    increased or remained the same.  The TPR could not have decreased, because
    all of the confidences for the true positives are still greater than or
    equal to the cutoff.  So, the ROC must be monotonically increasing.

    \textbf{(ii)} A majority class classifier would always predict the majority
    class, and it would do so with confidence equal to the proportion of the
    majority class.  When the confidence is below that proportion, the TPR is 0,
    but so is the FPR.  When the confidence threshold reaches that proportion,
    the classifier will do one of two things:
    \begin{itemize}
    \item If ``positive'' is the majority class, it predicts everything
      positive.  The true positive rate is 1, and the false positive rate is
      also 1.
    \item If ``negative'' is the majority class, it predicts everything
      negative.  The true and false positive rates are 0, since nothing is
      predicted positive anyway.
    \end{itemize}
    In either case, since we start ROC curves at the origin and draw them to
    $(1,1)$, we have a diagonal line.

    \textbf{(iii)} We assume that a random classifier would predict randomly
    with confidence of $0.5$.  Since the classifier is completely random, the
    proportion of positives that are predicted positive is $0.5$, and similarly
    for the FPR.  So, the ROC curve goes from $(0,0)$ to $(0.5, 0.5)$ to
    $(1,1)$, and is therefore a diagonal line.
  \end{problem}

  \begin{problem}{5}
    \begin{question}
      Derive the backpropagation weight updates for hidden-to-output and
      input-to-hidden weights when the loss function is cross entropy with a
      weight decay term.  Cross entropy is defined as
      $L(\vec{w}) = -\sum_i y_i \log \hat{y}_i + (1-y_i)\log(1-\hat{y}_i)$,
      where $y_i$ is true label (assumed 0/1) and $\hat{y}_i$ is the estimated
      label for the $i\textsuperscript{th}$ example. (10 points)
    \end{question}

    We will denote the overall loss function (with weight decay term) as
    $L_{OC}$.  Since $L_{OC} = L + \gamma \sum_{i,j} w_{ji}^2$, we have
    $\frac{\partial L_{OC}}{\partial w_{ji}} = \frac{\partial L}{\partial
      w_{ji}} + 2 \gamma w_{ji}$.
    So we will first compute $\frac{\partial L}{\partial w_{ji}}$ and then add
    in the $2\gamma w_{ji}$ term at the end.

    We will use the same variable conventions as presented in the slides.
    Assuming a sigmoid activation function $h(u)$, we have
    $\frac{dh}{du} = h(u)(1-h(u))$.  In order to compute
    $\frac{\partial L}{\partial w_{ji}}$, we do the same decomposition as
    presented in class:
    $\frac{\partial L}{\partial w_{ji}} = \frac{\partial L}{\partial n_{j}}
    \frac{\partial n_j}{\partial w_{ji}} = \frac{\partial L}{\partial n_j}
    x_{ji}$.
    We start with the output layer, computing $\frac{\partial L}{\partial n_j}$:

    \begin{align*}
      \frac{\partial L}{\partial n_j}\
      &= \frac{d}{dn_j} \left(L(y_j, \hat{y}_j) \right)\\
      &= \frac{d}{dn_j} \left(L(y_j, h(n_j)) \right)\\
      &= \frac{d}{dn_j} \left(-y_j \log h(n_j) - (1-y_j)\log(1-h(n_j)) \right)\\
      &= -\frac{y_j}{h(n_j)} \frac{dh(n_j)}{dn_j} - \frac{1-y_j}{1-h(n_j)} \left(- \frac{dh(n_j)}{dn_j}\right)\\
      &= -\frac{y_j}{h(n_j)} h(n_j)(1-h(n_j)) + \frac{1-y_j}{1-h(n_j)} h(n_j)(1-h(n_j))\\
      &= -y_j (1-h(n_j)) + (1-y_j)h(n_j)\\
      &= -y_j + y_j h(n_j) + h(n_j) - y_j h(n_j) \\
      &= h(n_j) - y_j
    \end{align*}

    This gives us the final partial derivatives for the outer layers.

    \begin{align*}
      \frac{\partial L}{\partial w_{ji}} &= (h(n_j) - y_j) x_{ji} \\
      \frac{\partial L_{OC}}{\partial w_{ji}} &= (h(n_j) - y_j) x_{ji} + 2
                                                \gamma w_{ji} \\
    \end{align*}

    For input-hidden layers (at node $i$), we do a similar process.  We start by
    finding $\frac{\partial L}{\partial n_j}$:

    \begin{align*}
      \frac{\partial L}{\partial n_j}
      &= \sum_{k\in Downstream(j)} \frac{\partial L}{\partial n_k}
        \frac{\partial n_k}{\partial n_j} \\
    \end{align*}

    Since $n_k = \sum_l w_{kl}h(n_l)$, we have:

    \begin{align*}
      \frac{\partial n_k}{\partial n_j} = w_{kj} h(n_j)(1-h(n_j))
    \end{align*}

    So,
    \begin{align*}
      \frac{\partial L}{\partial n_j}
      &= \sum_{k\in Downstream(j)} \frac{\partial L}{\partial n_k}
        \frac{\partial n_k}{\partial n_j} \\
      &= h(n_j)(1-h(n_j)) \sum_{k\in Downstream(j)} \frac{\partial L}{\partial
        n_k} w_{kj}\\
      &= h(n_j)(1-h(n_j)) \sum_{k\in Downstream(j)} \frac{\partial L}{\partial
        w_{kj}} \frac{w_{kj}}{x_{kj}}
    \end{align*}

    And finally, since
    $\frac{\partial L}{\partial w_{ji}} = \frac{\partial L}{\partial n_j}
    x_{ji}$, we have:

    \begin{align*}
      \frac{\partial L}{\partial w_{ji}}
      &= h(n_j)(1-h(n_j)) x_{ji} \sum_{k\in Downstream(j)} \frac{\partial L}{\partial w_{kj}} \frac{w_{kj}}{w_{kj}} \\
      \frac{\partial L_{OC}}{\partial w_{ji}}
      &= 2 \gamma w_{ji} + h(n_j)(1-h(n_j)) x_{ji} \sum_{k\in Downstream(j)} \frac{\partial L}{\partial w_{kj}} \frac{w_{kj}}{x_{kj}} \\
    \end{align*}
  \end{problem}

  The parameter update is simply
  $\vec{w} \gets \vec{w} - \eta \frac{\delta L_{OC}}{\delta \vec{w}}$, using the
  partial derivatives calculated above.

\end{document}