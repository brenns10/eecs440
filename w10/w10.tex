\documentclass[fleqn]{homework}

\student{Stephen Brennan (smb196)}
\course{EECS 440}
\assignment{Written 10}
\duedate{November 17, 2015}

\usepackage{enumerate}
\usepackage{mathtools}
%\usepackage{graphicx}

\begin{document}
  \maketitle

  \begin{problem}{1}
    \begin{question}
      The Bayesian Candy Factory makes a Halloween Candy Box that contains a mix
      of yummy (Y) and crummy (C) candy. You know that each Box is one of three
      types: 1. 75\% Y and 25\% C, 2.  50\% Y and 50\% C and 3. 25\% Y and 75\%
      C. You open a Box and start munching candies. Let the
      $i\textsuperscript{th}$ candy you munch be denoted by $c_i$. Answer the
      following questions using MATLAB, R or any other math package. Generate
      one Box with 100 candies for each type, and assume a fixed order of
      munching. (1) For each Box, plot $Pr(T=i|c_1,\dots,c_N)$ on a graph where
      $T$ represents a type and $N$ ranges from 1 to 100. (You should have three
      graphs and each graph will have three curves.) (2) For each Box, plot
      $Pr(c_{N+1}=C|c_1,\dots,c_N)$ where $N$ ranges from 1 to 99. (3) Suppose
      you are an optimist, and before opening a Box you believe that each Box
      has 75\% Y (type 1) with probability 0.8 and the probability of the other
      two types is 0.1 each. Redo part (1) taking this belief into
      account. Briefly explain the implications of your results. (15 points)
    \end{question}

    For this situation, I have defined the probability of a list of candies
    $c_1, \dots, c_N$ given a box type as binomial, defining success as a Yummy
    candy, and failure as a Crummy one.  So, for example,
    $Pr[c_1, \dots, c_N|T=1]$ is binomial with parameters $N$ and $0.75$.  This
    is a slight simplification, since it models drawing candies with replacement
    from a population having the given statistics.  In reality, what is
    happening is that the box is drawn from such a population, and then candies
    are drawn with replacement from that box.  However, I believe that this
    simplification is reasonable for the purposes of this question's
    demonstration.

    Given this assumption, to compute the probability of a particular box type
    given candies, we simply apply Bayes' Rule:

    \begin{equation}
      Pr[T=i | c_1, \dots, c_N] = \frac{Pr[c_1, \dots, c_N | T=i] Pr[T=i]}{\sum_{i=1}^3 Pr[c_1, \dots, c_N|T=i] Pr[T=i]}
    \end{equation}

    These conditional probabilities are computed using the binomial distribution
    above.  We assume that $Pr[T=i]=\frac{1}{3}$ in parts 1 and 2, and then
    update this assumption in part 3.

    To compute the probability of the next candy being crummy given the current
    list of candies, we simply condition on the type computed above:

    \begin{equation}
      Pr[c_{N+1} = C | c_1, \dots, c_N] = \sum_{i=1}^3 Pr[c_{n+1}=C|T=i] Pr[T=i|c_1, \dots, c_N]
    \end{equation}

    I apologize for the size of the figures presented below -- I attempted to
    present them in the most compact manner possible.  This document is
    completely scalable, so you can zoom in on the figures to see much better
    detail.

    \begin{enumerate}[1.] 
    \item (below) \item Plots for parts 1 and 2 are provided below.  For the box
      type 1 ($p(Y)=0.75$):

      \includegraphics[width=0.5\textwidth]{p1_part1_0_75.pdf}
      \includegraphics[width=0.5\textwidth]{p1_part2_0_75.pdf}

      For the box type 2 ($p(Y)=0.50$):

      \includegraphics[width=0.5\textwidth]{p1_part1_0_50.pdf}
      \includegraphics[width=0.5\textwidth]{p1_part2_0_50.pdf}

      For the box type 3 ($p(Y)=0.25$):

      \includegraphics[width=0.5\textwidth]{p1_part1_0_25.pdf}
      \includegraphics[width=0.5\textwidth]{p1_part2_0_25.pdf}

    \item When we adjust our priors, we obtain the following figures.  For the
      box type 1 ($p(Y)=0.75$):

      \includegraphics[width=0.65\textwidth]{p1_part3_0_75.pdf}

      For the box type 2 ($p(Y)=0.50$):

      \includegraphics[width=0.65\textwidth]{p1_part3_0_50.pdf}

      For the box type 3 ($p(Y)=0.25$):

      \includegraphics[width=0.65\textwidth]{p1_part3_0_25.pdf}
    \end{enumerate}

    As the figures above demonstrate, the ``optimism'' has a significant effect
    on the number of candies required to ``decide'' which type the box is.  When
    the box is type 1, the probability of box 1 is nearly exactly 1 after only
    20 candies, and it is significantly higher than the other probabilities for
    the entire curve.  Meanwhile, the other types take much longer to
    ``decide'', especially for type 2, since that is easiest to ``confuse'' with
    type 1.
  \end{problem}

  \begin{problem}{2}
    \begin{question}
      When estimating parameters for a Boolean attribute $f$ in a na\"ive Bayes
      model, it is observed that $f$ is true in $k$ out of $n$ positive
      examples. Further, there is a Dirichlet prior on the parameter
      representing the probability of $f$ given a positive example, with
      hyper-parameters $a$ and $b$. Show that the MAP estimate of the
      parameter's value is equivalent to an $m$-estimate with specific values of
      $m$ and $p$. In this way show that $m$-estimates act as Bayesian prior
      knowledge in na\"ive Bayes. The Dirichlet distribution over
      $0 \le \theta \le 1$ is given by:
      $D(\theta;a,b)=[(a-1)! (b-1)! /(a+b-1)!] \theta^{(a-1)} (1-
      \theta)^{(b-1)}$
      where $a$, $b$ are positive integers greater than 1 and are parameters of
      the distribution. (15 points)
    \end{question}

    The MAP estimate of $\theta$ is:

    \begin{align*}
      \hat{\theta} &= \arg\max_\theta Pr(D|\theta)Pr(\theta) \\
      &= \arg \max_\theta \theta^k (1-\theta)^{n-k} \frac{(a-1)!(b-1)!}{(a+b-1)!} \theta^{a-1} (1-\theta)^{b-1} \\
      &= \arg \max_\theta \theta^{k+a-1} (1-\theta)^{n-k+b-1} \\
      LL(\theta) &= (k+a-1)\log \theta + (n-k+b-1)\log(1-\theta) \\
    \end{align*}

    Next we maximize log odds:

    \begin{align*}
      \frac{dLL}{d\theta} = \frac{k+a-1}{\theta} - \frac{n-k+b-1}{1-\theta} &= 0 \\
      \frac{k+a-1}{\theta} &= \frac{n-k+b-1}{1-\theta} \\
      (1-\theta)(k+a-1) &= \theta(n-k+b-1) \\
      k+a-1-k\theta-a\theta+\theta &= n\theta-k\theta+b\theta-\theta \\
      k+a-1&=\theta(n+a+b-2)\\
      \hat{\theta} &= \frac{k+a-1}{n+a+b-2} \\
    \end{align*}

    The ML parameter estimate of $\theta$ with $m$-estimates would be
    $\theta=\frac{k+mp}{n+m}$.  We see that $m=a+b-2$, and therefore
    $p=\frac{a-1}{a+b-2}$.
  \end{problem}

  \begin{problem}{3}
    \begin{question}
      Consider a regression problem with examples described by 2 continuous
      attributes, $x$ and $y$.  Each example is sampled according to the uniform
      distribution on $(-1,1)^2$ and labeled with $f(x,y)=1 - x^2 - y^2$ . A
      learnerâ€™s hypothesis class is $h(x,y)=ax+by+c$. (a) Calculate its bias and
      variance as a function of $x$ and $y$ if the learner sees an arbitrarily
      large training sample. (b) Using MATLAB, find the $(x,y)$ with the largest
      bias and the $(x,y)$ with the largest variance for samples of size 10. Can
      you intuitively justify your findings? (15 points)
    \end{question}

    \begin{enumerate}[(a)]
    \item If the learner sees an arbitrarily large training sample, then it
      should converge to a $\hat{h}(x,y)$ that minimizes the total squared loss
      over all $(x,y) \in (-1,1)^2$.  That is, it should converge to:

      \begin{align*}
        \hat{h}(x,y) &= \arg \min_{h} \int_{-1}^1\int_{-1}^1 (h(x,y) - f(x,y))^2 dx dy \\
        &= \arg \min_{a,b,c} \int_{-1}^1\int_{-1}^1 (x^2 + ax + y^2 + by + c-1)^2 dx dy \\
      \end{align*}

      Using the table below, we expand out the multiplication.  For
      simplification, we make the substitution $d=c-1$.

      \begin{tabular}{l|lllll}
              &$x^2$     & $ax$     & $y^2$    & $by$    & $d$     \\
        \hline
        $x^2$ & $x^4$    & $ax^3$   & $x^2y^2$ & $bx^2y$  & $dx^2$ \\
        $ax$  & $ax^3$   & $a^2x^2$ & $axy^2$  & $abxy$   & $adx$  \\
        $y^2$ & $x^2y^2$ & $axy^2$  & $y^4$    & $by^3$   & $dy^2$ \\
        $by$  & $bx^2y$  & $abxy$  & $by^3$   &  $b^2y^2$ & $bdy$ \\
        $d$   & $dx^2$   & $adx$   & $dy^2$   &  $bdy$    & $d^2$ \\
      \end{tabular}

      Due to the impending integration, we can ignore anything that has an odd
      power of $x$ or $y$, since that will integrate to an even power and cancel
      out upon evaluating the definite integral.  So, this evaluates to:

      \begin{align*}
        \hat{h}(x,y) &= \arg \min_{a,b,c} \int_{-1}^1\int_{-1}^1 \left(x^4 + xy^4 + 2x^2y^2 + (a^2 + 2d)x^2 + (b^2 + 2d) y^2 + d^2 \right)dx dy \\
        &= \arg \min_{a,b,c} \int_{-1}^1 \left[\frac{1}{5}x^5 + y^4 + \frac{2}{3}x^3y^2 + \frac{1}{3}(a^2 + 2d)x^3 + (b^2 + 2d) xy^2 + d^2x \right]_{x=-1}^{x=1}dy \\
        &= \arg \min_{a,b,c} \int_{-1}^1 \left(\frac{2}{5} + 2y^4 + \frac{4}{3}y^2 + \frac{2}{3}(a^2+2d) + 2(b^2 + 2d)y^2 + 2d2\right)dy \\
        &= \arg \min_{a,b,c} \int_{-1}^1 \left( 2y^4 + \left(\frac{4}{3} + 2b^2 + 4d\right)y^2 + \frac{2}{5}+\frac{2}{3}(a^2+2d) + 2d^2\right)dy \\
        &= \arg \min_{a,b,c} \left[ \frac{2}{5}y^5 + \frac{1}{3}\left(\frac{4}{3} + 2b^2 + 4d\right)y^3 + \frac{2}{5}y+\frac{2}{3}(a^2+2d)y + 2d^2y\right]_{y=-1}^{y=1} \\
        &= \arg \min_{a,b,c} \frac{4}{5} + \frac{2}{3}\left(\frac{4}{3} + 2b^2 + 4d\right) + \frac{4}{5}+\frac{4}{3}(a^2+2d) + 4d^2 \\
        &= \arg \min_{a,b,c} \frac{4}{3}a^2 + \frac{4}{3}b^2 + 4d^2 + \frac{16}{3}d \\
        &= \arg \min_{a,b,c} \frac{4}{3}a^2 + \frac{4}{3}b^2 + 4c^2 - \frac{8}{3}c
      \end{align*}

      Setting the gradient to zero, we obtain $a=0$, $b=0$, and $c=\frac{1}{3}$.
      Therefore, $\hat{h}(x,y) = \frac{1}{3}$.

      Now that we have the hypotheses that a learner would have after training
      on an arbitrarily large sample, we can compute the bias and variance.  In
      general:

      \begin{align*}
        E[f(x,y) - h(x,y)] = V(h(x,y)) + V(f(x,y)) + [\bar{h}(x,y) - f(x,y)]^2
      \end{align*}

      The variance error is $V(h(x,y))$.  Since $h(x,y) = \frac{1}{3}$, this
      variance is zero.

      The bias error is
      $(\bar{h}(x,y) - f(x,y))^2 = (\frac{1}{3} + x^2 + y^2 - 1)^2 = (x^2 + y^2
      - \frac{2}{3})^2$

    \item To compute the variance error and bias error in general (in order to
      numerically solve this problem), we must first compute the expected value
      of $h$, and its variance.

      \begin{align*}
        E(h(x,y)) &= \frac{1}{4}\int_{-1}^1\int_{-1}^1 (ax + by + c) dx dy \\
        &= \frac{1}{4} \int_{-1}^1 \left[\frac{ax^2}{2} + bxy +
          cx\right]_{x=-1}^{x=1} dy \\
        &= \frac{1}{4} \int_{-1}^1 (2by + 2c)dy \\
        &= \frac{1}{4} \left[\frac{by^2}{2} + 2cy\right]_{y=-1}^{y=1} \\
        &= \frac{1}{4} * 4c = c
      \end{align*}

      My script first gets a random sample of 10 points from $(-1, 1)^2$.  It
      performs least squares regression to get values for $a$, $b$, and $c$.
      Then, it maximizes the bias error
      ($bias(x,y) = (\bar{h}(x,y) - f(x,y))^2 = (c + x^2 + y^2 - 1)^2$) and the
      variance error
      ($variance(x,y) = (\bar{h}(x,y) - h(x,y))^2 = (c - h(x,y))^2$).  In both
      situations, I get the bias and variance to be maximized when
      $(x,y) = (0,0)$.
    \end{enumerate}
  \end{problem}

  \begin{problem}{4}
    \begin{question}
      Suppose a learner uses bootstrap resampling to construct a training sample
      $T$ from an initial sample $U$, of the same size as $U$. Show that, for a
      large enough $U$, the probability that some example from $U$ appears in
      $T$ is approximately 0.63. (5 points)
    \end{question}

    Let the size of the initial sample be denoted by $n$.  The probability of a
    single example $u$ being in $T$ is:

    \begin{align*}
      Pr[u \in T] &= 1 - Pr[u \not\in T] \\
      &= 1 - \left(\frac{n-1}{n}\right)^n
    \end{align*}

    We take the limit as $n\to\infty$:

    \begin{align*}
      \lim_{n\to\infty} Pr[u \in T]
      &= \lim_{n\to\infty} \left(1 - \left(\frac{n-1}{n}\right)^n\right) \\
      &= \lim_{n\to\infty} 1 -  \lim_{n\to\infty}\left(\frac{n-1}{n}\right)^n \\
      &= 1 - \lim_{n\to\infty}\left(1 - \frac{1}{n}\right)^n \\
      &= 1 - \frac{1}{e} \approx 0.63
    \end{align*}

    The last step is a generally known special limit.  Thus, for suitably large
    sample size, the probability of a single example being present in the
    bootstrap sample is approximately 0.63.
  \end{problem}

\end{document}