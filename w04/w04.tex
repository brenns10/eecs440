\documentclass[fleqn]{homework}

\student{Stephen Brennan (smb196)}
\course{EECS 440}
\assignment{Written 4}
\duedate{September 22, 2015}

%\usepackage{mathtools}
%\usepackage{graphicx}

\begin{document}
  \maketitle

  \begin{problem}{1}
    \begin{question}
      Explain in your own words: \textbf{(i)} why memorization should not be
      considered a valid learning approach, \textbf{(ii)} why tabula rasa
      learning is impossible, and \textbf{(iii)} why picking a good example
      representation is important for learning.  Try to use good, intuitive
      examples from human learning to motivate your arguments. (10 points)
    \end{question}

    \textbf{(i)} Memorization is not a valid approach to learning because the
    memorized concept can not be applied to new examples of the same problem.
    When humans learn to do simple arithmetic, memorizing multiplication tables
    is a common starting point.  However, if you have memorized a multiplication
    table, you are powerless to multiply two numbers if one is not included in
    the table.  It's not until you learn how to do multiplication generally
    (which frequently uses the tables) that you have actually learned the
    concept of multiplication and can apply it to any two numbers.

    \textbf{(ii)} When creating a learning system, you must take into account
    the ``hypothesis space'' that your system will search.  It's not guaranteed
    that the actual concept your system should learn is contained within this
    space.  If you make your hypothesis space general enough to guarantee that
    the target concept is contained by it, you also include the concept that
    memorizes your examples, which will match the examples at least as well as
    the target concept, and probably better.  Your learning system will
    therefore always memorize rather than learn.  Therefore, you must use some
    sort of prior knowledge to restrict this hypothesis space.  This means you
    must start with some sort of knowledge or bias in your hypothesis space,
    instead of beginning from a blank slate (or tabula rasa).

    \textbf{(iii)} When a human learns multiplication, it's usually helpful to
    start with groups of small objects, demonstrating how multiplication is
    actually repeated addition.  Although this is possible to see with a normal
    representation of numbers, using small objects like coins, beans, or candy
    frequently makes the concept much more obvious to the human mind.  In the
    same way, when designing a machine learning system, it is important to give
    examples to the system in a way that will make the target concept more
    apparent to the system.
  \end{problem}

  \begin{problem}{2}
    \begin{question}
      Do you think it might be possible to have a ``best'' learning algorithm,
      that would outperform all other algorithms on all learning problems?
      Explain why or why not.  (Hint: think about the consequences of the
      proposition that tabula rasa learning is impossible.) (10 points).
    \end{question}

    Almost certainly such an algorithm would not be possible.  Expanding on my
    explanation of ``no tabula rasa learning'' from \textbf{(1.ii)}, in order to
    have an effective learning system that avoids memorization, you must
    restrict the hypothesis space of your algorithm.  In doing so, you create an
    inductive bias for your system that takes into account some of your prior
    knowledge about the problem.  A generalized algorithm could not come up with
    this restriction because it has no prior knowledge of the problems it would
    be applied to.

    Even if there were some automated way to come up with an appropriate
    inductive bias for a problem, designing an algorithm that can search some
    arbitrary subset of a huge hypothesis space is not trivial--a general
    approach would likely be inefficient.

    Finally, a learning algorithm is frequently heavily coupled with the example
    representation it uses.  Having an algorithm that works generally for any
    representation is unlikely at best.
  \end{problem}

  \begin{problem}{3}
    \begin{question}
      From first principles (without using other results), prove that in a
      binary classification task, the information gain $IG(X)$ for any binary
      split variable $X$ is always non negative. (10 points).
    \end{question}
  \end{problem}

  \begin{problem}{4}
    \begin{question}
      Show that for a continuous attribute $X$, the only split values we need to
      check to determine a split with max $IG(X)$ lie between points with
      different labels.  (Hint: consider the following setting for $X$: there is
      a candidate split point $S$ in the middle of $N$ examples with the same
      label, to the right of $n$ such examples.  To the left, there are $L_0$
      examples with label negative and $L_1$ with label positive, and likewise
      to the right.  Express the information gain of $S$ as a function of $n$.
      Then show that this function is maximized either when $n=0$ or $n=N$.) (20
      points)
    \end{question}
  \end{problem}

\end{document}