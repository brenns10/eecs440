\documentclass[fleqn]{homework}

\student{Stephen Brennan (smb196)}
\course{EECS 440}
\assignment{Written 4}
\duedate{September 22, 2015}

\usepackage{mathtools}
%\usepackage{graphicx}

\begin{document}
  \maketitle

  \begin{problem}{1}
    \begin{question}
      Explain in your own words: \textbf{(i)} why memorization should not be
      considered a valid learning approach, \textbf{(ii)} why tabula rasa
      learning is impossible, and \textbf{(iii)} why picking a good example
      representation is important for learning.  Try to use good, intuitive
      examples from human learning to motivate your arguments. (10 points)
    \end{question}

    \textbf{(i)} Memorization is not a valid approach to learning because the
    memorized concept can not be applied to new examples of the same problem.
    When humans learn to do simple arithmetic, memorizing multiplication tables
    is a common starting point.  However, if you have memorized a multiplication
    table, you are powerless to multiply two numbers if one is not included in
    the table.  It's not until you learn how to do multiplication generally
    (which frequently uses the tables) that you have actually learned the
    concept of multiplication and can apply it to any two numbers.

    \textbf{(ii)} When creating a learning system, you must take into account
    the ``hypothesis space'' that your system will search.  It's not guaranteed
    that the actual concept your system should learn is contained within this
    space.  If you make your hypothesis space general enough to guarantee that
    the target concept is contained by it, you also include the concept that
    memorizes your examples, which will match the examples at least as well as
    the target concept, and probably better.  Your learning system will
    therefore always memorize rather than learn.  Therefore, you must use some
    sort of prior knowledge to restrict this hypothesis space.  This means you
    must start with some sort of knowledge or bias in your hypothesis space,
    instead of beginning from a blank slate (or tabula rasa).

    \textbf{(iii)} When a human learns multiplication, it's usually helpful to
    start with groups of small objects, demonstrating how multiplication is
    actually repeated addition.  Although this is possible to see with a normal
    representation of numbers, using small objects like coins, beans, or candy
    frequently makes the concept much more obvious to the human mind.  In the
    same way, when designing a machine learning system, it is important to give
    examples to the system in a way that will make the target concept more
    apparent to the system.
  \end{problem}

  \begin{problem}{2}
    \begin{question}
      Do you think it might be possible to have a ``best'' learning algorithm,
      that would outperform all other algorithms on all learning problems?
      Explain why or why not.  (Hint: think about the consequences of the
      proposition that tabula rasa learning is impossible.) (10 points).
    \end{question}

    Almost certainly such an algorithm would not be possible.  Expanding on my
    explanation of ``no tabula rasa learning'' from \textbf{(1.ii)}, in order to
    have an effective learning system that avoids memorization, you must
    restrict the hypothesis space of your algorithm.  In doing so, you create an
    inductive bias for your system that takes into account some of your prior
    knowledge about the problem.  A generalized algorithm could not come up with
    this restriction because it has no prior knowledge of the problems it would
    be applied to.

    Even if there were some automated way to come up with an appropriate
    inductive bias for a problem, designing an algorithm that can search some
    arbitrary subset of a huge hypothesis space is not trivial--a general
    approach would likely be inefficient.

    Finally, a learning algorithm is frequently heavily coupled with the example
    representation it uses.  Having an algorithm that works generally for any
    representation is unlikely at best.
  \end{problem}

  \begin{problem}{3}
    \begin{question}
      From first principles (without using other results), prove that in a
      binary classification task, the information gain $IG(X)$ for any binary
      split variable $X$ is always non negative. (10 points).
    \end{question}

    The information gain $IG(X)$ with respect to class labels $Y$ is defined as:

    \begin{align*}
      IG(X) &= H(Y) - H(Y|X) \\
            &= H(Y) - \sum_{x \in X} p(x) H(Y|X=x) \\
            &= H(Y) + \sum_{x \in X} \sum_{y \in Y} p(x) p(y|x) \log p(y|x) \\
            &= H(Y) + \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x,y)}{p(x)} \\
            &= H(Y) + \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x,y) - \sum_{x \in x} \sum_{y \in Y} p(x,y) \log p(x) \\
            &= H(Y) - H(X,Y) - \sum_{x \in X} p(x) \log p(x) \\
            &= H(Y) + H(X) - H(X,Y)
    \end{align*}

    Here, $H(X,Y)$ is the joint entropy of the attribute $X$ and the class
    labels $Y$.  For any $X$ and $Y$, therefore, $IG(X) \ge 0$ iff
    $H(Y) + H(X) \ge H(X,Y)$.

    For binary $X$ and $Y$ (as the problem stipulates), the probability
    distribution of the joint variable $X,Y$ is defined by three probabilities:
    $p_x$ (probability $X$ is positive), $p_y$ (probability $Y$ is positive),
    and $p_{xy}$ (probability $X$ and $Y$ are both positive).  Using these three
    variables, we can write the probability distribution of $X,Y$:

    \begin{tabular}{l|ll}
           & $Y+$     & $Y-$ \\
      \hline
      $X+$ & $p_{xy}$ & $p_x - p_{xy}$ \\
      $X-$ & $p_y - p_{xy}$ & $1 - p_x - p_y + p_{xy}$ \\
    \end{tabular}

    Now, we will hold $p_x$ and $p_y$ constant, and find what value of $p_{xy}$
    (in terms of $p_x$ and $p_y$) maximizes $H(X,Y)$.  For ease of arithmetic,
    we will minimize $h = -H$.  So, we begin by writing out $h(p_{xy})$:

    \begin{align*}
      h(p_{xy}) =& -H(X,Y) = \sum_{x \in X} \sum_{y \in Y} p(x,y) \log p(x,y) \\
                =& p_{xy} \log p_{xy} + (p_x - p_{xy}) \log (p_x - p_{xy}) + (p_y - p_{xy}) \log (p_y - p_{xy})\\
                 &+ (1 - p_x - p_y + p_{xy}) \log (1 - p_x - p_y + p_{xy})\\
      \frac{dh}{dp_{xy}} =& \log p_{xy} - \log (p_x - p_{xy}) - \log (p_y - p_{xy}) + \log (1 - p_x - p_y + p_{xy}) \\
      =& \log \frac{p_{xy}(1-p_x-p_y+p_{xy})}{(p_x - p_{xy})(p_y-p_{xy})}\\
      \frac{d^2h}{dp_{xy}^2} =& \frac{1}{p_{xy}} + \frac{1}{p_x-p_{xy}} + \frac{1}{p_y-p_{xy}} + \frac{1}{1 - p_x - p_y + p_{xy}} \\
    \end{align*}

    We find critical points where $\frac{dh}{dp_{xy}}=0$:

    \begin{align*}
      0 &= \log \frac{p_{xy}(1-p_x-p_y+p_{xy})}{(p_x - p_{xy})(p_y-p_{xy})} \\
      1 &= \frac{p_{xy}(1-p_x-p_y+p_{xy})}{(p_x - p_{xy})(p_y-p_{xy})} \\
      p_{xy}(1-p_x-p_y+p_{xy}) &= (p_x - p_{xy})(p_y-p_{xy}) \\
      p_{xy} - p_x p_{xy} - p_y p_{xy} + p_{xy}^2 &= p_x p_y - p_x p_{xy} - p_y p_{xy} + p_{xy}^2 \\
      p_{xy} &= p_x p_y \\
    \end{align*}

    We calculate $\frac{d^2h}{dp_{xy}^2}$ at this point:

    \begin{align*}
      \frac{d^2h}{dp_{xy}^2} &= \frac{1}{p_{xy}} + \frac{1}{p_x-p_{xy}} + \frac{1}{p_y-p_{xy}} + \frac{1}{1 - p_x - p_y + p_{xy}} \\
      &= \frac{1}{p_{x}p_{y}} + \frac{1}{p_x(1-p_y)} + \frac{1}{p_y(1-p_x)} + \frac{1}{(1 - p_x)(1 - p_y)} \\
    \end{align*}

    Since $p_x$ and $p_y$ are probabilities, the whole thing is positive, and
    therefore this critical point minimizes $h$, and therefore maximizes
    $H(X,Y)$.  The value of $H(X,Y)$ at this point is:

    \begin{align*}
      H(X,Y) =& - \sum_{x \in X} \sum_{y \in Y} p(x,y) \log p(x,y) \\
             =& -p_x p_y \log (p_x p_y) - (p_x)(1-p_y) \log (p_x)(1-p_y) \\
              &- (1-p_x)(p_y) \log (1-p_x)(p_y) - (1-p_x)(1-p_y) \log (1-p_x)(1-p_y) \\
    \end{align*}

    When we separate out the log products and combine like terms, we find that
    $H(X,Y) = H(X) + H(Y)$ at its maximal value (for any $X$ or $Y$).
    Therefore, we can see that $H(X) + H(Y) \ge H(X,Y)$ is always true, and
    therefore, the information gain of $X$ is always positive.

    It is worth noting that there is probably a much shorter way to do this
    proof that I didn't come up with.  Also, this inequality $IG(X) \ge 0$ holds
    true for any $X$ and $Y$ (not just binary), due to the much more general
    statement that the mutual information between two discrete random variables
    is always non-negative.  Of course, I could not use that statement for this
    proof!
  \end{problem}

  \begin{problem}{4}
    \begin{question}
      Show that for a continuous attribute $X$, the only split values we need to
      check to determine a split with max $IG(X)$ lie between points with
      different labels.  (Hint: consider the following setting for $X$: there is
      a candidate split point $S$ in the middle of $N$ examples with the same
      label, to the right of $n$ such examples.  To the left, there are $L_0$
      examples with label negative and $L_1$ with label positive, and likewise
      to the right.  Express the information gain of $S$ as a function of $n$.
      Then show that this function is maximized either when $n=0$ or $n=N$.) (20
      points)
    \end{question}

    Using the hint in the question statement above, we write the information
    gain of $X$ split at $S$ as a function of $n$.  We assume without loss of
    generality that the $N$ examples are labeled positive.  Additionally, since
    $H(Y)$ is constant with respect to $n$, we disregard it in the information
    gain.

    \begin{align*}
      IG(X)
      =& H(Y) - H(Y|X) \\
      =& -p(X \le S) H(Y|X \le S) - p(X > S) H(Y|X > S) \\
      =& \frac{L_0+L_1+n}{L_0+L_1+N+R_0+R_1} \left(
           p(Y=0|X \le S) \log p(Y=0| X \le S) + p(Y=1|X \le S) \log p(Y=1|X \le S)
         \right)\\
       &+ \frac{R_0+R_1+N-n}{L_0+L_1+N+R_0+R_1} \left(
           p(Y=0|X > S) \log p(Y=0| X > S) + p(Y=1|X > S) \log p(Y=1|X > S)
         \right) \\
      =& \frac{L_0+L_1+n}{L_0+L_1+N+R_0+R_1} \left(
           \frac{L_0}{L_0+L_1+n} \log \frac{L_0}{L_0+L_1+n}
           + \frac{L_1+n}{L_0+L_1+n} \log \frac{L_1+n}{L_0+L_1+n}
         \right)\\
       &+ \frac{R_0+R_1+N-n}{L_0+L_1+N+R_0+R_1} \bigg(
         \frac{R_0}{R_0+R_1+N-n} \log \frac{R_0}{R_0+R_1+N-n}\\
       &+ \frac{R_1+N-n}{R_0+R_1+N-n} \log \frac{R_1+N-n}{R_0+R_1+N-n} \bigg) \\
    \end{align*}

    Simplifying further, we obtain:

    \begin{equation*}
      IG(X) = \frac{1}{L_0+L_1+N+R_0+R_1} \left(
    \begin{aligned}
      L_0 \log \frac{L_0}{L_0+L_1+n}+ &(L_1+n) \log \frac{L_1+n}{L_0+L_1+n}+ \\
      R_0 \log \frac{R_0}{R_0+R_1+N-n}+ &(R_1+N-n) \log \frac{R_1+N-n}{R_0+R_1+N-n}\\
    \end{aligned} \right)
    \end{equation*}

    Taking the derivative and simplifying, we obtain:

    \begin{equation*}
      \frac{dIG}{dn} = \frac{1}{L_0+L_1+N+R_0+R_1} \log \frac{(L_1+n)(R_0+R_1+N-n)}{(L_0+L_1+n)(R_1+N-n)}
    \end{equation*}

    The critical points of $IG$ are when $n=0$, $n=N$, and when this derivative
    is 0.  In order for the derivative to be $0$, the $\log$ must be 1.  Or, the
    numerator and denominator of the inside fraction must be the same:

    \begin{align*}
      (L_1+n)(R_0+R_1+N-n) &= (L_0+L_1+n)(R_1+N-n) \\
      (L_1+n)(R_1+N-n) + (L_1+n)R_0 &= (L_1+n)(R_1+N-n) + L_0(R_1+N-n) \\
      L_1 R_0 + R_0 n &= L_0 R_1 + L_0 N - L_0 n \\
      (R_0 + L_0) n &= L_0 R_1 + L_0 N - L_1 R_0 \\
      n &= \frac{L_0 R_1 + L_0 N - L_1 R_0}{R_0 + L_0} \\
    \end{align*}

    At this point, I would attempt to show that the second derivative evaluated
    at this point is positive, and therefore this point is a minimum, not a
    maximum.  With that, I could say that either $n=0$ or $n=N$ is the value
    that maximizes the information gain.  However, I believe there may be some
    error in my computation up to this point, because the next steps cannot be
    practically computed by hand.
  \end{problem}

\end{document}