\documentclass[fleqn]{homework}

\student{Stephen Brennan (smb196)}
\course{EECS 440}
\assignment{Written Homework 3}
\duedate{September 15, 2015}

\usepackage{mathtools}
%\usepackage{graphicx}

\begin{document}
  \maketitle

  \begin{problem}{1}
    \begin{question}
      Let $V(X)$ denote the variance of a random variable $X$.  From the
      definitions, prove that for any two independent random variables $X$ and
      $Y$, $V(X + Y) = V(X) + V(Y)$. (10 points)
    \end{question}

    \begin{align*}
      V(X + Y) &= E((X+Y-E(X+Y))^2) \\
               &= E(((X-E(X)) + (Y-E(Y)))^2) \\
               &= E((X-E(X))^2 + 2(X-E(X))(Y-E(Y)) + (Y-E(Y))^2) \\
               &= E((X-E(X))^2) + E(2(X-E(X))(Y-E(Y))) + E((Y-E(Y))^2) \\
               &= V(X) + V(Y) + 2E(XY - XE(Y) - YE(X) + E(X)E(Y)) \\
               &= V(X) + V(Y) + 2(E(X)E(Y) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y)) \\
               &= V(X) + V(Y)
    \end{align*}

    The critical step that needs justification here is going from $E(XY)$ to
    $E(X)E(Y)$, which is only true because $X$ and $Y$ are independent.
  \end{problem}

  \begin{problem}{2}
    \begin{question}
      A function $f$ is said to have a global minimum at $x$ if for all $y$,
      $f(y) \ge f(x)$.  It is said to have a local minimum at $x$ if there
      exists a neighborhood $H$ around $x$ so that for all $y$ in $H$,
      $f(y) \ge f(x)$.  Show that, if $f$ is convex, every local minimum is a
      global minimum.  [Hint: Prove by contradiction using Jensen's inequality.]
      (10 points)
    \end{question}

    Let us assume that $f$ is a convex function, and $x_1$ is a local minimum,
    but not a global minimum.  Since $x_1$ is not a global minimum, there exists
    some value $x_2$ such that $f(x_2) < f(x_1)$.  By Jensen's inequality,
    $f(\lambda x_1 + (1-\lambda) x_2) \le \lambda f(x_1) + (1-\lambda) f(x_2)$,
    for $0 \le \lambda \le 1$.  Since $x_1$ is a local minimum, there is a $y$
    in the neighborhood of $x_1$ which is between $x_1$ and $x_2$, for which
    $f(y) \ge f(x)$.  So, for $0 < \lambda_y < 1$ corresponding to $y$, we must
    have
    $f(\lambda_y x_1 + (1-\lambda_y) x_2) = f(y) \le \lambda_y f(x_1) +
    (1-\lambda_y) f(x_2)$.
    Since $f(x_2) < f(x_1)$ and $\lambda_y \ne 1$, we know that
    $\lambda_y f(x_1) + (1-\lambda_y) f(x_2) < f(x_1)$.  But this gives us the
    inequality $f(y) < f(x_1)$, which contradicts the earlier statement that
    $f(y) \ge f(x_1)$.  Therefore, if $f$ is a convex function, and $x$ is a
    local minimum, $x$ must also be a global minimum.
  \end{problem}

  \begin{problem}{3}
    \begin{question}
      Describe two learning tasks that might be suitable for machine learning
      approaches.  For each task, write down a goal, a possible performance
      measure, what examples you might get, and what a suitable hypothesis space
      might be.  What learning setting (supervised, unsupervised, etc.) seems
      most appropriate for each task?  What example representation seems most
      appropriate?  Be original -- don't write about tasks discussed in class or
      described in the text.  Preferably select tasks from your research area.
      Describe any aspect of the task(s) that may not fit well with the learning
      settings and representations we have discussed.  Especially interesting
      discussion may receive bonus points. (15 points)
    \end{question}

    \textbf{Task 1: Bioinformatics.}  Cancer is a very complex and heterogeneous
    disease.  Two cancers of the same organ may have entirely separate causes
    and mechanisms.  Therefore, when choosing treatments, it is important to
    choose those which target the mechanisms of the particular cancer you're
    treating.  Since the mechanisms and treatments of cancer are still an area
    of active research, it is important to be able to choose the best treatment
    regardless of whether the mechanisms have been pinned down yet.  This leads
    to the following machine learning problem: given clinical information about
    a cancer patient, choose a treatment that will maximize their lifetime.

    \begin{itemize}
    \item Performance measure: amount of time since diagnosis a patient
      survives.
    \item Examples: clinical data (somatic mutations, gene expression levels,
      demographic information, medical history, etc.) from previous cancer
      patients (readily available from sources such as The Cancer Genome Atlas).
    \end{itemize}

    This task lends itself to the feature vector representation of its examples.
    One potential hypothesis space would be the set of all available cancer
    treatments.  A more interesting hypothesis space could be the set of all
    subsets of cancer treatments (since many patients are treated with multiple
    drugs, it can be difficult to separate out the effects of each drug, and the
    results may in fact be due to that particular combination of drugs).
    However, neither of these hypothesis spaces would restrict the machine
    learning algorithms enough to prevent overfitting, so it would be important
    to emphasize cross validation or restricting the hypothesis space to a
    subset of the available treatments.

    Since there are already large databases of example data available, this
    problem would be well suited for a supervised learning algorithm.

    \textbf{Task 2: Advertising.}  To large retailers (especially online),
    making more sales is very important.  These retailers have a good deal of
    data available to them: namely, the lists of items that customers purchase
    in a single trip (or shopping cart).  They may be able to make more sales by
    advertising other products to customers as they shop.  A machine learning
    approach to this problem would be an excellent way to try to increase sales.

    \begin{itemize}
    \item Performance measure: amount of money spent by a customer, or perhaps
      the amount of items purchased by the customer.
    \item Examples: shopping baskets of previous customers.  Perhaps even more
      interesting would be if the retailer tracked what was already in a
      customer's cart when they added a new item.  In this case, the examples
      might be items along with the shopping cart contents they are added to.
    \end{itemize}

    This task would lend itself to a somewhat more complex representation of
    examples.  Since retailers may have vast amounts of products, a feature
    vector would be huge, and probably contain too many features to accurately
    predict any meaningful behavior from.  The examples might be better
    represented in something like Multiple-Instance Representation.  It may also
    be wise to try to reduce the number of features by using the category of the
    product, or by grouping all the similar products (of different brands) into
    the same feature.

    This advertising task would also be very good for unsupervised learning.
    The algorithm could take action by choosing advertisements to display to the
    user, and it could determine for itself whether its advertisement was
    successful by looking at its performance measure.  In this case, it may be
    even more useful to include in the performance measure things like whether
    or not the user clicked on the advertisement.
  \end{problem}

  \begin{problem}{4}
    \begin{question}
      Consider a learning problem where the examples are described by $n$
      Boolean attributes, and the hypothesis space is the space of all Boolean
      functions on $n$ variables.  How many distinct examples can you have in
      this setting?  How many distinct decision trees can you construct in this
      setting?  ``Distinct'' means that each tree must represent a different
      hypothesis in the space.  Give a rigorous justification for your
      answer. (8 points)
    \end{question}
  \end{problem}

  \begin{problem}{5}
    \begin{question}
      Show that the entropy of a Bernoulli random variable is a concave function
      (i.e. the negative entropy is a convex function). (7 points)
    \end{question}
  \end{problem}

\end{document}