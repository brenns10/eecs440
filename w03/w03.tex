\documentclass[fleqn]{homework}

\student{Stephen Brennan (smb196)}
\course{EECS 440}
\assignment{Written Homework 3}
\duedate{September 15, 2015}

\usepackage{mathtools}
%\usepackage{graphicx}

\begin{document}
  \maketitle

  \begin{problem}{1}
    \begin{question}
      Let $V(X)$ denote the variance of a random variable $X$.  From the
      definitions, prove that for any two independent random variables $X$ and
      $Y$, $V(X + Y) = V(X) + V(Y)$. (10 points)
    \end{question}

    \begin{align*}
      V(X + Y) &= E((X+Y-E(X+Y))^2) \\
               &= E(((X-E(X)) + (Y-E(Y)))^2) \\
               &= E((X-E(X))^2 + 2(X-E(X))(Y-E(Y)) + (Y-E(Y))^2) \\
               &= E((X-E(X))^2) + E(2(X-E(X))(Y-E(Y))) + E((Y-E(Y))^2) \\
               &= V(X) + V(Y) + 2E(XY - XE(Y) - YE(X) + E(X)E(Y)) \\
               &= V(X) + V(Y) + 2(E(X)E(Y) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y)) \\
               &= V(X) + V(Y)
    \end{align*}

    The critical step that needs justification here is going from $E(XY)$ to
    $E(X)E(Y)$, which is only true because $X$ and $Y$ are independent.
  \end{problem}

  \begin{problem}{2}
    \begin{question}
      A function $f$ is said to have a global minimum at $x$ if for all $y$,
      $f(y) \ge f(x)$.  It is said to have a local minimum at $x$ if there
      exists a neighborhood $H$ around $x$ so that for all $y$ in $H$,
      $f(y) \ge f(x)$.  Show that, if $f$ is convex, every local minimum is a
      global minimum.  [Hint: Prove by contradiction using Jensen's inequality.]
      (10 points)
    \end{question}

    Let us assume that $f$ is a convex function, and $x_1$ is a local minimum,
    but not a global minimum.  Since $x_1$ is not a global minimum, there exists
    some value $x_2$ such that $f(x_2) < f(x_1)$.  By Jensen's inequality,
    $f(\lambda x_1 + (1-\lambda) x_2) \le \lambda f(x_1) + (1-\lambda) f(x_2)$,
    for $0 \le \lambda \le 1$.  Since $x_1$ is a local minimum, there is a $y$
    in the neighborhood of $x_1$ which is between $x_1$ and $x_2$, for which
    $f(y) \ge f(x)$.  So, for $\lambda_y$ corresponding to $y$, we must have
    $f(\lambda_y x_1 + (1-\lambda_y) x_2) \le \lambda_y f(x_1) + (1-\lambda_y)
    f(x_2)$.
    Since $f(x_2) < f(x_1)$, we know that
    $\lambda_y f(x_1) + (1-\lambda_y) f(x_2) < f(x_1)$.  But since
    $\lambda_y x_1 + (1-\lambda_y) x_2 = y$, this gives us the inequality
    $f(y) < f(x_1)$, which contradicts the earlier statement that
    $f(y) \ge f(x_1)$.  This contradiction proves the original statement.
  \end{problem}

  \begin{problem}{3}
    \begin{question}
      Describe two learning tasks that might be suitable for machine learning
      approaches.  For each task, write down a goal, a possible performance
      measure, what examples you might get, and what a suitable hypothesis space
      might be.  What learning setting (supervised, unsupervised, etc.) seems
      most appropriate for each task?  What example representation seems most
      appropriate?  Be original -- don't write about tasks discussed in class or
      described in the text.  Preferably select tasks from your research area.
      Describe any aspect of the task(s) that may not fit well with the learning
      settings and representations we have discussed.  Especially interesting
      discussion may receive bonus points. (15 points)
    \end{question}
  \end{problem}

  \begin{problem}{4}
    \begin{question}
      Consider a learning problem where the examples are described by $n$
      Boolean attributes, and the hypothesis space is the space of all Boolean
      functions on $n$ variables.  How many distinct examples can you have in
      this setting?  How many distinct decision trees can you construct in this
      setting?  ``Distinct'' means that each tree must represent a different
      hypothesis in the space.  Give a rigorous justification for your
      answer. (8 points)
    \end{question}
  \end{problem}

  \begin{problem}{5}
    \begin{question}
      Show that the entropy of a Bernoulli random variable is a concave function
      (i.e. the negative entropy is a convex function). (7 points)
    \end{question}
  \end{problem}

\end{document}