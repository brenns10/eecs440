\documentclass[fleqn]{homework}

\student{Stephen Brennan (smb196)}
\course{EECS 440}
\assignment{Written 6}
\duedate{October 6, 2015}

\usepackage{mathtools}
\newcommand{\sigmoid}{\:\text{sigmoid}}
%\usepackage{graphicx}

\begin{document}
  \maketitle

  \begin{problem}{1}
    \begin{question}
      Suggest modifications for backpropagation for non-feedforward neural
      network structures: \textbf{(i)} Edges are allowed between nodes in the
      same layer as well as between successive layers, but the graph is still
      directed acyclic. In other words, nodes in layer $k$, $x_{k1}$, $x_{k2}$,
      ..., $x_{kn}$ can have edges between them as well as to the k+1 layer, as
      long as no cycle is created. \textbf{(ii)} Edges are allowed from a layer
      to preceding layers but not into the input layer, so the graph is directed
      cyclic, but the input layer only has outgoing edges. (10 points)
    \end{question}

    \textbf{(i)} So long as there are no cycles, you can go layer by layer, and
    backpropagate the nodes of each layer in an ordering such that each
    successor node is backpropagated before its predecessor.  More generally,
    for any directed acyclic graph, doing backpropagation in the order produced
    by a topological sort will always work.

    \textbf{(ii)} When cycles are introduced, backpropagation can no longer work
    by doing a single pass over the network of nodes.  Instead, you could use an
    iterative approach.  Each iteration, do a pass over every node (probably
    starting at the output layers) computing estimates of the partial
    derivatives.  When you require a partial derivative you haven't computed
    yet, use the previous iteration's estimate (or a random number during the
    first iteration).  Repeat this process until the partial derivative
    estimates for the input layer ``converge'' given some $\epsilon$.
  \end{problem}

  \begin{problem}{2}
    \begin{question}
      Consider a neural network with a single hidden layer with sigmoid
      activation functions and a single output unit also with a sigmoid
      activation, and fixed weights. Show that there exists an equivalent
      network, which computes exactly the same function, where the hidden unit
      activations are the $tanh$ function described in class, and the output
      unit still has a sigmoid activation. (10 points)
    \end{question}

    Consider the hyperbolic tangent function:

    \begin{align*}
      \tanh(u, a, b) &= a\frac{e^{bu} - e^{-bu}}{e^{bu} + e^{-bu}} \\
                     &= a\left(\frac{e^{bu}}{e^{bu}}\right) \frac{e^{bu} - e^{-bu}}{e^{bu} + e^{-bu}} \\
                     &= a\frac{e^{2bu} - 1}{e^{2bu} + 1} \\
                     &= a\frac{e^{2bu} + 1}{e^{2bu} + 1} - a\frac{2}{e^{2bu} + 1} \\
                     &= a - 2 a\frac{1}{1 + e^{2bu}} \\
    \end{align*}

    When $a = b = -\frac{1}{2}$:

    \begin{align*}
      \tanh\left(u, -\frac{1}{2}, -\frac{1}{2}\right)
      &= -\frac{1}{2} + \frac{1}{1 + e^{-u}} \\
      &= -\frac{1}{2} + \sigmoid(u) \\
    \end{align*}

    Using this, we could replace each sigmoid hidden unit with a tanh hidden
    unit where $a = b = -\frac{1}{2}$, whose output will only differ by a
    constant of $-\frac{1}{2}$.  This will cause the input to the output unit's
    sigmoid function to be $w \cdot (-\frac{1}{2} + x)$, which is off by a
    constant value of $-w \cdot (\frac{1}{2}, \frac{1}{2}, ...)$ In order to
    eliminate this difference, we can simply add a dummy input to the output
    unit which is always 1.  We set the weight of that input to
    $(\frac{1}{2}, \frac{1}{2}, \dots) \cdot w$, and the resulting network will
    produce the exact same function as the original network.
  \end{problem}

  \begin{problem}{3}
    \begin{question}
      Draw an artificial neural network structure which can perfectly classify
      the examples shown in the table below. Treat attributes as
      continuous. Show all of the weights on the edges. For this problem, assume
      that the activation functions are sign functions instead of
      sigmoids. Propagate each example through your network and show that the
      classification is indeed correct. (10 points)

      \begin{tabular}{|rr|c|}
        \hline
        $x_1$ & $x_2$ & Class \\
        \hline
        -4 & -4 & - \\
        -1 & -1 & + \\
        1 & 1 & + \\
        4 & 4 & - \\
        \hline
      \end{tabular}
    \end{question}

    My network is shown below.  Weights are labeled next to their edges, and
    $\sigma$ values are labeled above their units.  Since $x_2$ provides no
    additional information, I left it out of my network!
    \begin{center}
      \includegraphics[width=0.6\textwidth]{net.pdf}      
    \end{center}

    Propagated examples: \\

    \begin{tabular}{|rr|rrrrr|}
      \hline
      $x_1$ & $x_2$ & $u_1$ & $u_2$ & $u_3$ & $u_4$ & $u_5$ \\
      \hline
      -4 & -4 & -1 & -1 & 1 & -1 & -1 \\
      -1 & -1 & -1 & 1 & -1 & -1 & 1 \\
      1 & 1 & -1 & 1 & -1 & -1 & 1 \\
      4 & 4 & 1 & 1 & -1 & 1 & -1 \\
      \hline
    \end{tabular}
  \end{problem}

  \begin{problem}{4}
    \begin{question}
      Using R/Matlab/Mathematica/your favorite math software, plot the decision
      boundary for an ANN with two inputs, two hidden units and one output. All
      activation functions are sigmoids.  Each layer is fully connected to the
      next. Assume the inputs range between −5 to 5 and fix all activation
      thresholds to 0. Plot the decision boundaries for the weights except the
      thresholds randomly chosen between \textbf{(i)} $(−10,10)$, \textbf{(ii)}
      $(−3,3)$, \textbf{(iii)} $(−0.1,0.1)$ (one random set for each case is
      enough). Use your plots to show that weight decay can be used to control
      overfitting for ANNs. (If you use Matlab, the following commands might be
      useful: meshgrid and surf). (10 points)
    \end{question}
  \end{problem}

  \begin{problem}{5}
    \begin{question}
      When learning the weights for the perceptron, we dropped the $sign()$
      activation function to make the objective smooth. Show that the same
      strategy does not work for an arbitrary ANN.  (Hint: consider the shape of
      the decision boundary if we did this.) (10 points)
    \end{question}
  \end{problem}

\end{document}